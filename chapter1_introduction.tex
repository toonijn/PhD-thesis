% !TeX root = chapter1_introduction.tex
\input{util.tex}
\startchapter{1}

\undefinedlabel{cha:c2}{2}
\undefinedlabel{cha:c3}{3}
\undefinedlabel{cha:c4}{4}

\chapter{Introduction to differential equations}

Differential equations recall in most mathematicians many feelings. Some only shiver by the mere idea of them, some of my colleagues in the more algebraic, (finite) geometric or discrete fields come to mind. Others, myself included, rejoice at the thought of studying them.

The knowledge about and experience with differential equations vary wildly among even the best of mathematicians. Some only had one, maybe two, introductory courses, others have studied them their whole careers. Important note: there are very few mathematicians that can say they have studied `differential equations'. Neither can I: I have studied \emph{a} differential equation; maybe two, if you count generously.

Before diving into the required mathematical background, let us take a step back. Mathematics does not live in a vacuum. Modern ideas have grown out of a very rich history, with many influences from the scientific questions of the time. In this introduction we will take the time to appreciate this history, and discover how differential equations have been developed.

\section{History}

In this section, we will walk through an abridged version of the history of differential equations. Before talking about functions, it is important to talk about what makes up these functions.

A good starting point is to take a look at numbers. Counting things is universal and timeless, as such the natural numbers $\NN$ are a jumping point to all the other numbers. In modern mathematics, the next logical step is to introduce the integers $\ZZ$. Historically however, only throughout the Middle Ages became negative numbers and the concept of zero widely accepted as numbers themselves. The first texts about negative numbers are from Chinese mathematicians somewhere between the second and seventh century AD. Later on, Indian and Islamic mathematicians calculated with negative numbers as well.

In historic texts, we find already references to (positive) fractions, as early as Ancient Egypt, around 1000 BC (maybe even earlier). So after $\NN$ the next numbers that were `discovered', were the positive rational numbers $\QQ^+$. Soon thereafter, some irrational numbers were found. The most prominent example is probably $\sqrt{2}$, the length of the diagonal of the unit square. But it is a bit too generous to say that this was the discovery of the real numbers. It is more accurate to say that there was a (geometric) notion of algebraic numbers $\QQbar$. These are the numbers that are solutions of polynomial equations, like $x^2 = 2$. However, only around the year 900, the Egyptian mathematician Abū Kāmil Shujā ibn Aslam started to accept these solutions as numbers in and of itself.

The mathematical invention that may have had the most impact in our daily lives may also be unnoticed by many: the Hindu--Arabic numeral system. This is a way of writing down numbers, invented between the first and fourth century. Most numeral systems were not made for large numbers or were cumbersome to work with. Only the best of mathematicians could do computation in those systems; especially multiplication was difficult. The Hindu--Arabic numeral system solved this by being positional based. This means that the last digits is the one's place, the digit before that ten's, then hundred's, and so on. This allows for a compact way to write down large numbers that still allows easy computation through arithmetic manipulations. Our modern numeral system is a direct descendant of the Hindu--Arabic numeral system. Compare for example the Roman numeral \uppercase\expandafter{\romannumeral 3846\relax} to our modern equivalent $3846$. And to help illustrate this point, try squaring both numbers.

From the sixteenth century onwards, mathematics in Europe started to flourish. For our purposes, the next stride towards the real numbers was made by Simon Stevin from Bruges. He created a way to write real numbers as a decimal expansion. It was Stevin who introduced the concept of `digits after the decimal point'. In this same century the complex numbers were discovered. And later in 1637, it was René Descartes who coined the terms `real' and `imaginary' numbers.

Yet, it still took more than 200 years before we would get a first formal mathematical definition of the real numbers $\RR$. It was the work of the great logicians of the late 19th century, spearheaded by Georg Cantor. He provided in 1874 a formal logical construction of the real numbers. It may seem surprising that a rigorous definition of the real numbers came so late in the rich history of mathematics. But, in practice, the development of calculus, and the theory of functions, did not really require strict formal definitions to advance.

\subsection{Calculus}

From a natural sciences view, it is natural to study relations between quantities, for example position in function of time, or air resistance in function of velocity. Yet, the earliest uses of calculus seem to have their origins in geometry: finding the area under a parabola, or the volume of a cone for example.

Throughout Ancient times and the Middle Ages, across mathematical texts of many cultures, diverse examples of what we now call derivatives and integrals can be found. But all these examples were concrete applications to problems. Only in the seventeenth century were the first unifying theories of derivatives and integrals of functions developed. It was by none other than Newton and Leibniz. The history between these two fathers of calculus is much richer than one would expect, it reads almost as a screenplay. In short, it is now accepted that both mathematicians independently developed the theory of calculus; at the time it was not.

The notations
$$
    \int f(x) \, \dd x \quad \text{ and } \quad \dv[]{f(x)}{x}
$$
for the integral and the derivative of a function $f$ were introduced by Leibniz. Newton introduced the notation
$$
    \dot{f}
$$
for the derivative of $f$. In mathematical texts this notation is less common, but it can be abundantly found in physics. In this work we will mostly use $\dv[]{f}{x}$ for the derivative of $f$. If the context makes the variable to which the derivative is taken clear, then we will abbreviate $\dv[]{f}{x}$ to $f'$.

For many examples of how the theory of calculus and especially differential equations have developed throughout the last 400 years, we refer the interested reader to~\cite{archibald_history_2005}. One of these examples explains what Newton probably understood by integrals and quadratures, and how these were computed. Another example is concerned with how the works of d'Alembert evolved.

\todo{Next two paragraphs: This could be improved, I'd recommend using the historical notes of the textbook of Simmons (I have it and will give it)\cite{simmons_differential_2016}}

The first differential equations, as we know them now, were studied shortly after Newton's work. Leibniz himself, and the Bernoulli brothers came across differential equations in the context of geometry and mechanics. In the eighteenth century the multivariate theory of Leibniz was extended and the first partial differential equations were written down. In the nineteenth century from the theoretical side, more results about the existence of solutions to differential equation were found. From the practical side, many more applications were found. Not only in mechanics, but also in heat transfer, optics, fluid flow and electricity and magnetism were differential equations instrumental. The equations of Navier and Stokes or the laws of Maxwell, to name a few, were developed.

The developments from the nineteenth century continued into the twentieth. Great new theoretical strides were made, and many new applications were found. Most notably, for this thesis at least, in quantum mechanics with Schrödinger's equation.

\subsection{Advent of computing}

Most\footnote{Mathematically, we can even say `almost all'.} differential equations cannot be explicitly solved. For ordinary linear differential equations with constant coefficients, for example, symbolic results are available. But if one of the coefficients is not a constant, these formulae fail. For this reason, numerical methods for differential equations were developed. These methods do not solve the equation in the strictest sense, but they are able to approximate solutions arbitrarily accurate. The first numerical method is Euler's method\footnote{Both the forward Euler method as the backward version.} from 1768. Albeit simple, in the nineteenth century Cauchy proved it to converge. This convergence assures, that if sufficiently small steps are taken, any accuracy for the approximated solution can be obtained.

Later, in the middle of the nineteenth century multistep methods were developed. And at the end of that century, Runge published the first Runge--Kutta method. These `new' methods were of higher order than the simpler methods and therefore could take larger steps to reach sufficiently accurate results.

What strikes me the most is that all these methods were developed long before computers were invented. This implies that the first applications of these methods required an unfathomable amount of manual computation. Now, we are spoiled, and computers do all the numerical heavy lifting for us.\footnote{Regularly I feel very fortunate to be a PhD-student now, and not then. I assume that, before computers, many tedious computations must have been assigned to students.} Back then, the term `computer' did exist, but it was a profession, not a device.

Throughout the twentieth century, with the rise of computing devices, the use and applicability of these numerical methods for ordinary differential equations exploded. We were able to reach never-before-seen accuracies in fractions of the time it would have costed only a few decades earlier. This explosion in computational power still continues. Now, the speed of affordable consumer desktop computers can be measured in teraFLOPS\footnote{`FLOPS' is a measure of how many \emph{fl}oating point \emph{o}perations \emph{p}er \emph{s}econd a computer can execute. A computer with a speed of $1$ teraFLOPS can execute one trillion floating point operations each second.}. For example, my university computer (Intel i7-8700K and an NVIDIA Quadro P2000) has a combined rated speed of a little over 3 teraFLOPS. Almost all of this speed comes from its graphics processing unit, with the CPU only contributing $0.06$ teraFLOPS. This already indicates that FLOPS is an imperfect unit. Writing a program that is able to use \emph{all} these FLOPS is prohibitively difficult.

\section{Ordinary differential equations}

In the most general formulation, an $n$-dimensional $k^\text{th}$ order \emph{ordinary differential equation}
\begin{equation}\label{equ:c1_ode_def}
    \vb{f}\left(x, \vb{y}, {\pdv[]{\vb{y}}{x}}, {\pdv[2]{\vb{y}}{x}}, \dots, {\pdv[k]{\vb{y}}{x}}\right) = \vb{0}
\end{equation}
expresses $n$ constraints on an unknown sufficiently differentiable function $\vb{y} : \Omega \to \RR^n$ with $\Omega \subseteq \RR$ and $\vb{f}: \Omega \times (\RR^n)^{k+1} \to \RR^n$. In most cases, this equation alone is insufficient to determine a unique solution $\vb{y}(x)$. For this, some initial or boundary conditions can be imposed on $\vb{y}$ (or any of the derivatives). As this abstract description is not intuitive, we provide some examples.

\paragraph{Integrals} The problem of determining primitive functions $F(x)$ can be rewritten as a one-dimensional first order ordinary differential equation:
$$
    F(x) = \int f(x) \,\dd x \quad \iff \quad \dv[]{F}{x} = f(x)\text{.}
$$
Definite integrals can similarly be formulated as a differential equation, however a boundary condition has to be imposed:
$$
    \int_a^b f(x) \dd x = F(b) \quad \text{ with } \dv[]{F}{x} = f(x) \text{ and } F(a) = 0\text{.}
$$

Mathematically, this is a valid example. Intuitively however, this seems rather trivial.

\paragraph{Swinging pendulum} Due to Newton's laws of motion, physical systems often give rise to second order ordinary differential equations. One such example is a pendulum. Here, a mass is suspended such that it can swing freely back and forth. Let $\theta(t)$ be the angle by which the mass is displaced from its hanging resting position at time $t$. The movement of the pendulum is described by
$$
    \dv[2]{\theta}{t} + \frac{g}{l} \sin(\theta) = 0\text{,}
$$
with $g$ the gravitational constant, and $l$ the distance between the fixed point and the mass. As initial conditions at time $t_0$, $\theta(t_0)$ represents the displacement angle at $t_0$ and $\theta'(t_0)$ represents some initial angular velocity with which the mass is released.

\paragraph{Brachistochrone curve} Mathematicians love challenging each other. This is true today, as much as it was 300 years ago. In 1696, Johann Bernoulli posed the Brachistochrone problem: on which curve slides a frictionless bead the fastest from point $A$ to $B$? There are many strategies for solving this problem. In one such solution, the following differential equation arises:
$$
    {\dv[]{y}{x}} = \sqrt{\frac{D - y}{x}} \text{ with $y(0) = 0$,}
$$
with $D$ a parameter.
% Image?

\paragraph{Lotka--Volterra equations} In biological modelling, dynamical systems can describe many real-world situations. The Lotka--Volterra equations make up one such system. This set of equations models the population size of two species. One function $r(t)$ expresses the population size of prey (for example rabbits), the other function $f(t)$ tracks the population size of the predatory species (for example foxes). The Lotka--Volterra equations are
$$
    \begin{cases}
        \dv[]{x}{t} = \alpha x - \beta x y  \\
        \dv[]{y}{t} = \delta x y - \gamma y \\
    \end{cases}\text{.}
$$
In this two-dimensional first order ordinary differential equation, the constants $\alpha$, $\beta$, $\gamma$ and $\delta$ are parameters which describe the particular characteristics of the situation. Typically, as initial conditions, a researcher supplies the population sizes of both species at the start of the simulation.

\subsection{Sturm--Liouville problems}

These were only a few short examples from a myriad of real-world applications. In chapter~\ref{cha:c2} we will focus upon the numerical solution of a particular class of second-order linear ordinary differential equations: Sturm--Liouville equations.

A Sturm--Liouville problem is defined by three functions $p(x)$, $q(x)$ and $w(x)$ on a domain $\Omega \subseteq \RR$.
$$
    -{\dv[]{}{x}}\left(p(x) \dv[]{y}{x}\right) + q(x) y = \lambda w(x) y
$$
This problem is only well-defined if appropriate boundary conditions are provided as well. For more details about different types of boundary conditions we refer the reader to the introduction in chapter~\ref{cha:c2}.

One striking particularity of these problems is that, besides $y(x)$, $\lambda$ is an unknown as well. Solutions for $\lambda$ are called eigenvalues and the corresponding solution for $y(x)$ is what is called an eigenfunction.

Differential equations can be approached from many angles. Some researchers take a pure theoretical approach, where many powerful results are proven. Other researchers start with real-world applications and consider differential equations as simply a tool in the toolbox to tackle their problems. The approach we have chosen falls somewhere in between. From the beginning we set out to develop and implement numerical methods for particular differential equations. To build a useful and accurate method, many theoretical results are needed. However, also practical feasibility and computational efficiency are essential.\footnote{Personally, I feel this is the true strength of this thesis. Neither the theoretical advancements we made, nor the efficiency of our implementation are perfect. However, I believe the powerful combination of the two to be the real innovation.}


\section{Partial differential equations}

One of the main characterizing properties of ordinary differential equations is that there is only a single independent variable. In equation~\eqref{equ:c1_ode_def}, $x$ is this independent variable. A \emph{partial} differential equation is a generalization which allows multiple independent variables. As these equations are commonly used in physical problems, the independent variables are mostly time $t$ and space $x, y, z, \dots$, or more general $x_1, x_2, \dots, x_n$.

Providing a clean general one-formula definition of partial differential equations is quite cumbersome, and not at all instructive. Therefore, let us immediately take a look at some examples.

\paragraph{The heat equation} Conductive heat transfer (among many other diffusive phenomena) is described by the heat equation. Let $f(t, x)$ be the temperature along an insulated metal rod with $x \in [0, 1]$ and $t \in \left[0, \infty\right[$. Physical laws dictate that this function satisfies
$$
    {\pdv[]{f}{t}} = \alpha \pdv[2]{f}{x}\text{.}
$$
In this expression, $\alpha$ is a parameter. As is the case for ordinary differential equations, to uniquely define a partial differential problem, boundary and or initial conditions have to be imposed. If for example the rod is heated at one side to $500\,\text{K}$, and cooled at the other to $250\,\text{K}$, then this can be expressed as $f(t, 0) = 500$ and $f(t, 1)=250$ for all $\left]0, \infty\right[$. However, this is still insufficient to properly define the problem. Initial conditions have to be provided for $t=0$. These should express the temperature throughout the rod at the beginning of the simulation. For example, if the experiment is started with the rod at room temperature $300\,\text{K}$, then this is captured as $f(0, x) = 300$ for all $x \in [0, 1]$.

This problem can be easily generalized to higher dimensional situations. Assume a similar experiment is conducted with a metal unit sphere. Let $g : \left[0, \infty\right[ \times \Omega : (t, x, y, z) \to g(t, x, y, z)$ be the temperature throughout this sphere $\Omega := \left\{(x, y, z)^\transposesign \in \RR^3 | x^2 + y^2 + z^2 \leq 1\right\}$. Physics dictate this $g$ satisfies:
$$
{\pdv[]{g}{t}} = \alpha\left(\pdv[2]{g}{x} + \pdv[2]{g}{y} + \pdv[2]{g}{z}\right) = \alpha \nabla^2 g \text{.}
$$
Again, appropriate boundary and initial conditions have to be specified.

\paragraph{Gauss's law for magnetism} Classical electromagnetism states that no magnetic monopoles exist. Mathematically this can be expressed as: at all times, the magnetic field $\vb{B} : \RR^3 \to \RR^3$ is divergence-free. Writing this as a partial differential equation makes this clear:
\begin{equation}\label{equ:c1_gauss_magnets}
    \nabla \cdot \vb{B} = 0 \quad\iff\quad \pdv[]{\vb{B}}{x} + \pdv[]{\vb{B}}{y} + \pdv[]{\vb{B}}{z} = 0\text{.}    
\end{equation}

This is a first-order three-dimensional linear partial differential equation. It illustrates that many laws and theories in physics can be formulated as (partial) differential equations.

In engineering, computer models of real-world situations are invaluable in making decisions. All of these models have to be based on (our understanding of) the world around us and the physical laws governing it. Partial differential equations are therefore essential in many of these models. If one wants to simulate magnetism, then equation~\eqref{equ:c1_gauss_magnets} may be sufficient. However, if electricity is also involved, then the more general (and much more complicated) equations of Maxwell have to be solved.

% Image?

\paragraph{Incompressible Navier--Stokes equations} To simulate the flow of incompressible fluids\footnote{The flow of water is a straightforward example. Also, these equations can handle slow flowing wind, as pressure can be assumed constant. If pressure cannot be neglected, then the compressible equations should be used.}, the incompressible Navier--Stokes equations can be used. Let $\vb{u} : \RR \times \RR^3 \to \RR^3$ express the flow vector at each moment $t$ in each point $(x, y, z)$. For incompressible flow, $\vb{u}$ has to satisfy
\begin{align*}
    & \pdv[]{\vb{u}}{t} = \nabla^2\vb{u} - (\vb{u}\cdot \nabla)\vb{u} + \vb{f} \\
    &\quad\text{ while }\nabla\cdot \vb{u} = 0\text{.}
\end{align*}

As one may already suspect, for most\footnote{Again, mathematically, we can say `almost all'.} partial differential equations, numerical methods need to be developed. In state-of-the-art modelling software, many tools are available. As in many of these programs complicated geometries are supported (such as the blades of a wind turbine, or the CAD-model of a bridge for example), numerical simulation methods based upon finite element techniques are preferred. These methods create a triangular mesh (or tetrahedral mesh for three-dimensional problems) of the domain, and use approximations on each of the cells of this mesh. In general, these techniques can be employed for an extremely diverse set of problems, albeit with an exceptionally high computational cost.

\subsection{Time-independent Schrödinger problems}

In this thesis, and in particular chapters~\ref{cha:c2} and~\ref{cha:c3}, we will be solving two-dimensional time-independent Schrödinger equations~\cite{schrodinger_quantisierung_1926}. These are defined by a potential $V(x, y)$ on a domain $\Omega \subseteq \RR^2$. The goal is to find all functions $\psi : \Omega \to \CC$ and values $E \in \CC$ such that
$$
-\nabla^2\psi + V(x, y) \psi = E \psi\text{,}
$$
with appropriate boundary conditions.

As many partial differential equations, the Schrödinger equation has its origins in physics. More specifically, the time-dependent Schrödinger equation describes wave functions in quantum mechanics. As I am a mathematician, I am not at all qualified to give any introduction to such an immensely complicated physics subject. Fortunately, in this thesis we are not concerned with the `why'\footnote{This is an easy-to-make statement as a mathematician. However, it is a dangerous statement for a scientist (which we most definitely are). Being blind to the applications is immoral. Quantum mechanics, and more generally our understanding of physics, has two sides. On the one hand, it obviously improves uncountable many lives immeasurably, directly and maybe even more so indirectly. On the other hand, not all human inventions have been for the better. The most incomprehensibly destructive weapons, for example, have only been possible due to advances in nuclear physics. As we are developing algorithms and methods for solving Schrödinger equations, implicitly (and by acknowledging this, now explicitly as well) we put our trust in the researchers who will be using our tools. So, to any user benefitting from our work, let this footnote be my plea to use it morally and responsibly.} of this equation; we are only trying to solve it.

\section{Notation}

Throughout this thesis we try to follow common notations present in the literature. However, different authors prefer different notations. For example from a physics perspective, $\bra{f}\ket{g}$ is extremely common. In mathematical analysis this notation is rare. There, $\langle f, g \rangle$ or $f \cdot g$ are more frequently used. For clarity, in this section we give an overview of the notational conventions used in this work.

\begin{description}[leftmargin=4cm, labelsep=0cm, style=multiline, itemsep=2mm, topsep=6mm]
    \item [$\begin{array}{@{}l} \NN, \ZZ, \QQ, \RR, \CC\\\NN^+, \QQ^+, \RR^+\end{array}$] The set of the natural numbers, the integers, the rational numbers, the real numbers and the complex numbers respectively. A superscript plus indicates the subset of only strictly positive numbers.
    \item [$\begin{array}{@{}l} a, b, c, \dots \\ \alpha, \beta, \gamma, \dots \end{array}$] Lowercase variables are assumed to be scalars.
    \item [$\vb{u}, \vb{v}, \dots$] Lowercase bold variables are assumed to be vectors.
    \item [$\vb{B}, \vb{M}, \vb{\Lambda}, \dots$] Uppercase bold variables are assumed to be matrices.
    \item [$\displaystyle\|\vb{x}\|_p $] The $L_p(\RR^n)$-norm: $\left(\sum_{i=1}^{n} \left|x_i\right|^p\right)^\frac{1}{p}$.
    \item [$\displaystyle\|\vb{x}\| = \|\vb{x}\|_2$] By default, norms are assumed to be Euclidean.
    \item [$\nabla f = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots\right)^\transposesign$] The gradient of a function is computed with respect to the space variables (not time).
    \item [$\Delta f = \nabla^2 f = \nabla \cdot \nabla f$] The Laplacian of a function is calculated as ${\pdv[2]{f}{x_1}} + {\pdv[2]{f}{x_2}} + \dots$.
    \item [$\dOmega$] For a domain $\Omega \in \RR^n$, the boundary $\dOmega$ is defined as $\overline{\Omega} \cap \overline{\Omega^c}$ (this is the intersection of the closure of $\Omega$ with the closure of the complement of $\Omega$.).
    \item [$\displaystyle\bra{f\,}\ket{\,g}$] For functions $f, g : \Omega \to \CC$ defined on $\Omega \subseteq \RR^n$, their inner product is calculated as $\int_\Omega f(\vb{x}) \overline{g(\vb{x})} \,\dd \vb{x}$.
\end{description}


\stopchapter
